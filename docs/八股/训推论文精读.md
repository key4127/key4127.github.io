## Megatron-LM

- 层内并行

- 仔细注意BERT类模型中层归一化的位置对于在模型尺寸增长时实现性能提升至关重要

不需要新编译器/更改，与流水线模型正交互补

### 成就

- 我们通过对现有的 PyTorch Transformer 实现进行少量针对性的修改，实现了一种简单高效的模型并行方法。
- 我们对模型和数据并行技术进行了深入的实证分析，并证明使用 512 个 GPU 可实现高达 76% 的扩展效率。
- 我们表明，在类似 BERT 的模型中仔细关注层归一化的位置对于在模型增长时实现更高的准确率至关重要。
- 我们证明了，增加模型规模可以提高 GPT-2（研究规模高达 83 亿参数）和 BERT（研究规模高达 39 亿参数）模型的准确率。

### 背景

节省显存：梯度检查点

- 数据并行：数据集分成子集分配给不同节点（GPU），每个节点都有完整的模型副本，计算梯度后通过集合通信同步更新（简单/通信开销大）
- 模型并行：模型不同层分到不同GPU（复杂/通信开销大/效率低）
- 参数共享：

模型并行：
- 逐层流水线并行：模型切分为多个阶段，分配到不同节点形成流水线（适用深度模型/复杂/可能效率低）
- 分布式张量计算：将大型张量切分，每个节点处理子张量，集合通信合并结果

### 并行方法

Megatron-LM并行的模型

- MLP：矩阵乘法、激活、乘法，中间结果较大，维度会扩展。第一个乘法按照扩展维度拆分（不改变隐藏层维度），通讯减少到1/k。
- 注意力：将不同的头（q、k）分到不同节点；v相应拆分，最后结果相加。

## DeepSpeed

### ZeRO-DP

### ZeRO-R
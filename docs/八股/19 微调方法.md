---
sidebar_position: 1
---

## 微调方法

在全参数、LoRA、QLoRA三种方式中，消耗显存由高到低递减；全参数微调训练速度最慢，QLoRA其次，LoRA最快；全参数微调有灾难性遗忘风险。  

### 全参数微调

全参数微调会调整预训练的所有参数，显存极高（参数量的4倍），所需数据量极大。理论上线高，适合复杂任务；但容易导致灾难性遗忘（模型之前的能力退化）。  

适用于计算资源充足且对模型性能要求极高的情况。

### LoRA（低秩适配）

令模型的原参数为矩阵$W_0$，LoRA对其进行微调，使新参数矩阵$W = W_0 + \Delta W$。这个变化量矩阵$\Delta W$是低秩的。  

低秩矩阵可由两个小矩阵相乘表示，即$W = BA$，其中矩阵$B$维度为$d \times r$，矩阵$A$维度为$r \times k$，$r$为矩阵的秩，为非常小的数（4、8、16）。训练只调整$A$和$B$的参数，参数量约为原来的0.1%到1%。  

在微调中，原参数会被冻结。具体而言，原参数会被标记，在反向传播时，会直接跳过被标记的张量，不计算该张量的梯度。初始化时，$A$用高斯初始化，$B$零初始化，保证最初$\Delta W$为零（训练起点与原模型完全相同）。

r较大时，可训练参数更多、显存占用更大、微调能力更强；但对应的，也会增加过拟合风险。可以在最初微调时采取较大r，在收敛后减小r或采用其它正则化方法。一般来说，可以先选取r=8作为基线。  

LoRA的目标模块指被微调的参数矩阵。在Transformer架构中，最常被选择的是注意力机制中的权重矩阵（Q、V）。

### QLoRA（量化低秩适配）

QLoRA是在LoRA的基础上进一步降低显存。在注入LoRA适配器并训练前，它将（全部）模型参数由16位浮点数量化为4位整数并加载到GPU显存，将显存减少四倍；此外，QLoRA采用了双量化技术，将已量化的常数再压缩一次；最后，采用了分页优化器，将优化器状态从GPU显存转移致CPU内存，进一步减少显存占用，允许训练更长序列，防止训练因显存不足而崩溃。  

FP16浮点数由符号位（1位）、指数位（5位）、尾数位（10）位组成，值域约为$1e^{-5}$到65504；4位整数的值域为-8到7。QLoRA使用分块量化策略：将每64/128个参数分为一块，计算出最大值和最小值，将其缩放到4位整数的值域。具体而言  

$$
缩放因子：S = (max(block) - min(block)) / (MAX_{INT4} - MIN_{INT4}) 
$$

$$
x = round((x - min(block)) / S + MIN_{INT4}) 
$$

然后，只需存储量化的各参数、缩放因子和最小值。  

在训练和推理时，需要将4位整数恢复为16位浮点数。这一过程会导致一定程度的精度损失，但损失被限制在了小块内，保存了模型整体的性能。每个小块的量化误差仅由该小块自身的数值范围决定。一个极端值只会在它所在的小块内引起较大的量化误差，而不会影响到其他块。  

双量化中，第二次量化的对象是第一次量化得到的缩放因子。所有缩放因子被视为一个独立的张量，对它们进行再次缩放。不缩放最小值的原因是最小值的值域分布可能更广、更不规则；此外，会带来额外的开销。  

优化器负责通过梯度信息更新参数值。它需要存储模型参数，并额外维护一阶矩、二阶矩两个状态，需要额外显存。分页优化器采取了类似虚拟内存的分页思想，将不常用的数据存储在CPU内存，在需要时换页。

另外，量化只针对原来的参数。为保持训练效果，新增的A、B矩阵需要保持较高精度，不会被量化。  

## 显存计算

### 全参数微调显存

全参数微调的显存需求涉及模型参数、梯度、优化器状态、激活值四个维度。  

$$
显存占用 = 参数显存 + 梯度显存 + 优化器状态显存 + 激活值显存
$$

参数、梯度为FP16，每个各需要两个字节。大部分操作在FP16参数上进行（计算梯度等），但更新时实际更新FP32参数，并将其转为FP16参数传给下一层，即还需存储FP32参数；以Adam优化器为例，需要动量、方差、参数副本三个FP32浮点数，每个参数对应需要12字节。此时显存公式为  

$$
显存占用 \approx 参数量 \times (2 + 2 + 12) + 激活值（字节）
$$

激活值与批次大小和序列长度正相关，公式为  

$$
激活值显存 = batch\_size \times seq\_len \times hidden\_size \times layer \times 12
$$

此处计算得到的激活值占用显存为理论值。实际使用梯度检查点优化后，显存会下降。梯度检查点技术是指前向传播时只存储检查点的激活值，反向传播时，如遇到未保留的激活值，重新计算。此外，还可以通过混合精度训练和ZeRO-3进一步优化。  

具体来说，7B模型至少需要8张A100（80GB）显卡；13B模型至少需要16张A100显卡。  

综合来看，有公式  

$$
全参数微调显存 = 参数量 \times (2 + 2 + 12) \times 1.2（冗余系数）+ batch\_size \times seq\_len \times 0.4MB
$$

冗余系数用于弥补其它开销，例如PyTorch框架开销、其它小规模的临时张量等。

### LoRA参数量计算

$$
可训练参数 = r \times (d\_input + d\_output) \times 层数
$$

这里的$d\_input$和$d\_output$分别代表矩阵A、B秩以外的另一个维度。  

LoRA不需要存储全部预训练参数的副本、梯度、方差等等，但仍然需要用FP16存储预训练参数本身。对于可训练参数，则需要存储全部副本、梯度等等。

### QLoRA参数量计算

$$
总显存 = 量化后模型显存 + 全参数微调显存 + 激活值显存
$$

其中参数微调缓存指LoRA部分需要微调的参数，每个参数分配16字节（包含副本、梯度等）。通过量化，模型预训练参数所占显存大约只有四分之一，双量化、分页会进一步降低显存。
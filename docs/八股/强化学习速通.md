### RLHF

基于人类反馈的强化学习：监督微调、训练奖励模型、强化学习微调

### PPO

近端策略优化算法：PPO会设置目标散度，希望每次更新都足够改变策略但稳定。如果散度（当前策略和微调的模型）超过目标1.5倍，则加重惩罚；如果更新太小，则扩大信任区域。

变体：PPO clip，直接限制策略改变范围。它在目标函数上进行限制，保证新旧策略比值处于一定范围。

如果仍然相差太远，采用提前停止的策略。

### DPO

绕过奖励这一步骤，直接利用人类偏好。

训练模型和参考模型的偏离程度本身就是奖励函数。

### GRPO

可以看作PPO的变种；不需要价值模型。每次基于当前的策略生成一组答案，为每个回答打分，
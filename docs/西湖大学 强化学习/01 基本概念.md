---
sidebar_position: 1
---

## 01 基本概念

这堂课基本上就是对概念的梳理。

### 部分基本概念

- trajectory（轨迹）：state-action-reward链条

- return：轨迹得到的所有奖励之和，可以是无限的

- discount rate：折扣因子

- discounted return：折扣回报

$$
discounted~return = r_1 + \gamma * r_2 + \gamma^2 * r_3 + ...
$$

- episode（回合/片段）：智能体从开始到结束的轨迹

- episodic task：通常认为是有限的任务

- continuing task：无限任务

### 马尔可夫链（MDP）

- 集合：状态、行动、对应奖励
- 概率分布：
    - 状态转移概率（行动后到达不同状态的概率分布）
    - 奖励概率（行动后获得奖励的概率分布）
- 策略
- 马尔可夫性质：未来与过去无关（对两个概率分布均成立）

$$
\begin{align*}
& p(s_{t + 1}|a_t, s_t, ..., a_0, s_0) = p(s_{t + 1}|a_t, s_t) \\
& p(r_{t + 1}|a_t, s_t, ..., a_0, s_0) = p(r_{t + 1}|a_t, s_t)
\end{align*}
$$
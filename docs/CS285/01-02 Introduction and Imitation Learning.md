---
sidebar_position: 1
---

## Lecture 01: Introduction

### 强化学习的概念

|特征|监督学习|强化学习|
|:-:|:-:|:-:|
|数据|独立同分布|不独立；先前的输出影响后续输入|
|答案|有真实答案|没有真实答案，只有“好坏”（奖励）|

## Lecture 02: Imitation Learning

### 术语与性质

|字母表示|含义|
|:-:|:-:|
|$s_t$|状态|
|$o_t$|观测|
|$a_t$|行动|
|$\pi_\theta(a_t\|s_t)$|（完全可观测环境下的）策略|
|$\pi_\theta(a_t\|o_t)$|策略|

可以认为观测是策略的子集。如果观测和策略相等，则处于完全可观测环境。

强化学习满足马尔可夫性质（未来只与现在有关）。

### 基本概念

模仿学习表面与监督学习类似；给定输入和期望的轨迹，让模型对应学习。  

和传统监督学习不同的地方在于，决策问题不是独立同分布的。在轨迹的某个位置，模型一旦犯错，就会很大可能偏离训练轨迹，积少成多导致最终远远偏离预期目标。  

解决方案包括：  

- 采用合适的数据
- 使用更强的模型，减少错误源头
- 多任务学习
- 改变算法（DAgger）

### 目标与下限

$$
c(s_t, a_t) = \begin{cases}
0, & a_t = \pi^*(s_t),\\
1, & otherwise
\end{cases}
$$

设定cost后，目标即为  

$$
minimize~E_{st\sim p_{\pi_\theta}(s_t)}[c(s_t, a_t)]
$$

此外，$r(s_t, a_t) = -c(s_t, a_t)$，所以最小化代价即为最大化奖励。

假设每次的偏差概率为 $\epsilon$。

$s \sim p_{train}(s)$指s是服从p训练真实分布的一个实例，而机器学习得到的实际概率分布和真实分布不等同。具体的，有  

$$
p_\theta(s_t) = (1 - \epsilon)^t p_{train}(s_t) + (1 - (1 - \epsilon)^t)p_{mistake}(s_t)
$$

移项，则有  

$$
|p_\theta(s_t) - p_{train}(s_t)| = (1 - (1 - \epsilon)^t)|p_{mistake}(s_t) - p_{train}(s_t)|
$$

由于对于概率$\epsilon$，有  

$$
(1 - \epsilon)^t \ge 1 - \epsilon t
$$

则得到  

$$
|p_\theta(s_t) - p_{train(s_t)} \le 2\epsilon t|
$$

而总的cost期望等于每个时间段中，所有状态的概率乘每个状态的cost，再求和

$$
\Sigma_t E_{p_\theta (s_t)}[c_t] = \Sigma_t \Sigma_{s_t} p_\theta(s_t)c_t(s_t)
$$

$$
原式 \le \Sigma_t \Sigma_{s_t} p_{train}(s_t)c_t(s_t) + |p_\theta(s_t) - p_{train}(s_t)|c_{max}
$$

$$
原式\le \Sigma_t \epsilon + 2\epsilon t
$$

由此可知，最坏可能性下，错误的程度为$O(\epsilon T^2)$。

尽管微小的错误可能带来严重后果，但模仿学习仍可以从错误中恢复。

### 解决方案

#### 数据

可以在数据中添加错误与修正的样例；还可以添加部分虚假的、包含修正过程的数据。

#### 模型

如果模型本身的能力足够强大，$\epsilon$足够小，那么平方的误差也可以忽略。

模型的行为一般是满足马尔可夫性质的；对于某种情况，不论先前发生过什么，模型都会选择同样的决策。这一行为和人类行为不同。这种情况会带来误差。此外，由于causal confusion，单纯地利用可能反而会降低模型性能。

解决这种情况有几种方案：  

- 混合高斯分布
- 潜变量模型（从观测数据抽出隐含变量）
- 扩散模型（逐步增加噪声构建数据，训练模型去除噪声）

此外，对于维度灾难问题（维度增加，性能下降、复杂度增加、数据稀疏性加剧），可以使用自动回归离散化处理。模型将动作每一维离散成若干个动作，将前一维选择的动作和状态作为后一维的输入，最终得到所有维度的动作，就得到了最终的action。

#### 多任务学习

与其直接训练一个$\pi_\theta(a|s)$策略，不如增加目标作为状态，训练$\pi_theta(a|s,g)$策略。 

此外，也可以不局限于模仿学习，从随机策略开始，选择随机目标的数据，训练、提升策略、重复。这种方式得到的策略具有很强的泛化能力。

#### 算法

要让模型得出的结果更接近现实，可以训练更强大的模型；但是同时也可以训练数据。

DAgger指Dataset Aggregation。具体行为如下：  

1. 通过标注数据集$D$训练$\pi_\theta(a_t|o_t)$
2. 运行$\pi_\theta$得到数据集$D_\pi$
3. 人类为数据标注action
4. 将数据集从原数据集$D$改为$D \cup D_\pi$

原始DAgger的问题是，人类并非每时每刻都在进行决策，所以人类的标注可能不准确，出现反事实因果的问题。

### 问题

模仿学习大量依赖人类的数据；此外，对于某些情况，人类不一定能给出合适的决策。更进一步地，模型应该有自主学习的能力。
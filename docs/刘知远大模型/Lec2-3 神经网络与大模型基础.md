---
sidebar_position: 1
---

## 神经网络基础

### 基础概念与训练

神经（neuron）是一个计算单元。神经包括n维的输入x、n维权重w、1维偏置b和激活函数f。从神经扩展到神经网络时，有 $a = f(Wx + b)$ 。权重与输入和输出相关；偏置只与输出相关。  

在多层神经网络中，只关注最开始的输入层和最后的输出层。中间的状态为隐藏态。

*激活函数的作用?*

不同类型的激活函数：  

![activation functions](./img/activation%20functions.png)

*为什么需要多层？*

给出N个例子x、y，分别对应神经网络的输入和输出。$F_\theta$为训练的神经网络函数，$\theta$为神经网络参数。l为损失函数，用来度量当前神经网络与目标间的距离。训练的具体目标为  

$$
min \sum l(y_i - F_\theta(x_i)) / N
$$

常见的求解参数的方法包括梯度下降法。将上述最小化函数写为 $J(\theta)$，求$J(\theta)$的梯度。

$$
\theta^{new} = \theta^{old} - \alpha \nabla J(\theta)
$$

其中 $\alpha$ 为学习率，太小导致进度慢，太大导致可能走的太远，找不到对应点。

使用梯度下降时，需要对J求$\theta$导数，也就需要对F求$\theta$导数。

求模型输出到输入的导数时，需要链式法则求每一层的导数，所以要求所有激活函数都可导；此外，激活函数输出0时，导数也为0，不利于训练，实际使用时可能需要微调。

神经网络需要与实际语言联系。例如，一个句子出现的概率等于数个条件概率相乘（马尔科夫链）。  

*独热编码*

### 循环神经网络（RNN）

RNN可以识别与序列相关的语言模式。  

在上文的神经网络中，有$h_i = f(Wh_{i - 1} + b)$。在RNN中，每个隐状态会额外地获取一个输入x，即$h_i = f(W_xx_i + W_hh_{i - 1} + b)$，并给出一个额外的输入y，即$y_i = F(h_i)$。 

*RNN衍生架构：LSTM、GRU*

*计算较慢*

实际应用中，句子的每个词通过独热编码转换为w，w通过embedding转化为向量x，作为RNN的输入。最终过了n个状态后，会产生（embedding的逆操作，将向量变为编码）下一个词作为输出。  

*softmax函数*

序列太长时，RNN会很慢。CNN是RNN的退化版本，只关注最近n个词对未来语言的影响，速度比RNN快，需要的记忆也更少，所需时间和序列长度无关。

*更快处理n-gram*

### Transformer

此处以机器翻译为例；这个例子需要记忆长序列。  

使用RNN时，使用Seq2Seq模型。顾名思义，这个模型由两个顺序RNN组成；第一个RNN输入所有的词汇，得到一个输出矩阵作为第二个RNN的输入矩阵，每次生成一个字，再将这个字作为新的输入求下一个字，进行翻译过程。

Transformer是更好的Seq2Seq架构，引入了attention机制，有 

$$
attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

Transformer架构可以支持并行和变长序列，并且仍然可以记录序列的所有信息。 

*Decoder-only与Encoder-Decoder*

*此处课上讲得不太清晰，具体看其它部分中的论文和对应笔记*

## 大模型基础

传统模型只具有处理一部分训练过的任务的能力，现在一般采用预训练模型。预训练模型的训练分为预训练、微调、测试三个部分。

### 概念

大模型在某种程度上是基于参数的迁移（微调阶段会调整参数）。

自监督是一种训练范式，不需要人工标注数据，而是利用数据来生成监督信号。通过各种方法，可以从数据中自动生成对应的“标签”，不需要手工标注。

### 语言模型

*CBOM、Skip-Gram*

*Word2Vec*

自然语言具有二义性。实际的向量库中，某个词的向量是所有含义向量的“总和”。实际使用时，需要根据上下文剔除一部分词义，使用更新后的向量继续完成下游任务。